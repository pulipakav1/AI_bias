{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qualitative Bias Probing\n",
        "\n",
        "Use this notebook to inspect the most toxic or stereotyped generations per model. Update the configuration cell below if you want to point to a different results file or change how many examples to inspect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "No results_with_metrics_*.csv files found in 'results/'.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m         display(Markdown(\u001b[33m\"\u001b[39m\u001b[33m**Most stereotyped responses**\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     41\u001b[39m         display(top_stereo[[\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstereotype_score\u001b[39m\u001b[33m\"\u001b[39m]])\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m metrics_df = \u001b[43mload_metrics_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m show_top_examples(metrics_df)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mload_metrics_dataframe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m     metrics_files = \u001b[38;5;28msorted\u001b[39m(RESULTS_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33mresults_with_metrics_*.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metrics_files:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo results_with_metrics_*.csv files found in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mresults/\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     metrics_path = metrics_files[-\u001b[32m1\u001b[39m]\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metrics_path.exists():\n",
            "\u001b[31mFileNotFoundError\u001b[39m: No results_with_metrics_*.csv files found in 'results/'."
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# ---------------------------------------------------------------------------\n",
        "RESULTS_DIR = Path(\"results\")\n",
        "METRICS_FILE = None  # Set to a specific CSV path to override auto-discovery\n",
        "TOP_K = 5            # Number of examples to show per model/metric\n",
        "\n",
        "def load_metrics_dataframe():\n",
        "    if METRICS_FILE:\n",
        "        metrics_path = Path(METRICS_FILE)\n",
        "    else:\n",
        "        metrics_files = sorted(RESULTS_DIR.glob(\"results_with_metrics_*.csv\"))\n",
        "        if not metrics_files:\n",
        "            raise FileNotFoundError(\"No results_with_metrics_*.csv files found in 'results/'.\")\n",
        "        metrics_path = metrics_files[-1]\n",
        "    if not metrics_path.exists():\n",
        "        raise FileNotFoundError(f\"Metrics file not found: {metrics_path}\")\n",
        "    print(f\"Loading metrics from: {metrics_path}\")\n",
        "    return pd.read_csv(metrics_path)\n",
        "\n",
        "\n",
        "def show_top_examples(df: pd.DataFrame):\n",
        "    required_cols = {\"model\", \"dataset\", \"prompt\", \"response\", \"toxicity_score\", \"stereotype_score\"}\n",
        "    missing = required_cols - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "    for model, model_df in df.groupby(\"model\"):\n",
        "        display(Markdown(f\"## {model}\"))\n",
        "\n",
        "        top_toxic = model_df.sort_values(\"toxicity_score\", ascending=False).head(TOP_K)\n",
        "        display(Markdown(\"**Most toxic responses**\"))\n",
        "        display(top_toxic[[\"dataset\", \"prompt\", \"response\", \"toxicity_score\"]])\n",
        "\n",
        "        top_stereo = model_df.sort_values(\"stereotype_score\", ascending=False).head(TOP_K)\n",
        "        display(Markdown(\"**Most stereotyped responses**\"))\n",
        "        display(top_stereo[[\"dataset\", \"prompt\", \"response\", \"stereotype_score\"]])\n",
        "\n",
        "metrics_df = load_metrics_dataframe()\n",
        "show_top_examples(metrics_df)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
